
## Classic manual gradient descent

В данном задании курса Start ML (Karpov Kourses) предлагается реализовать градиентный спуск без использования библиотечного функционала для оптимизации коэффициентов линейной регрессии методом наименьших квадратов.

В качестве решения написан класс GradientDescentMse с разделенной логикой реализации  между функциями внутри класса.
Для наглядности созданы графики с траекториями обучения градиентного спуска в зависимоти от подбираемых параметров длины шага и значения threshold.

### Ключевые аспекты реализации:

1. **Матричная форма вычислений** 
   - использование матричных операций вместо циклов для производительности

2. **Критерии остановки**:
   - Достижение порога `threshold` (длина шага β < threshold)
   - Превышение `max_iter` итераций

3. **Инициализация весов**:
   - Используется единичная инициализация: `beta = np.ones(n_features)`

 ###  Элементы реализации

 **Атрибуты:**
- `beta` - вектор коэффициентов (весов) модели
- `learning_rate` - скорость обучения (шаг градиентного спуска)
- `threshold` - порог остановки (критерий сходимости)
- `max_iter` - максимальное количество итераций
- `samples`, `targets` - данные для обучения

**Методы:**
1. `add_constant_feature` - добавление фичи для интерсепта
2. `calculate_mse_loss` - вычисление MSE на текущих параметрах
1. `calculate_gradient()` - подсчет градиента функции потерь
2. `iteration()` - выполнение одной итерации градиентного спуска
4. `predict()` - предсказание target (также на новых данных)
5. `learn()` - итеративное обучение весов модели